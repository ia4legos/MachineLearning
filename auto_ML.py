# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ouWdD_hfb8hSlpNdJSdN2vI-KHk5Uwye
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np          # importamos numpy como np
import pandas as pd         # importamos pandas como pd
import math
import random
import warnings
# Ignorar advertencias de convergencia para los modelos lineales
warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')

# Cargamos módulos de análisis gráficos
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
sns.set_theme(style = 'whitegrid')
# %config InlineBackend.figure_format = 'retina'

# Preprocesado
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
# División muestras
from sklearn.model_selection import train_test_split

# Modelos de clasificación
from sklearn.linear_model import LogisticRegression, RidgeClassifier, LinearRegression, Lasso, Ridge
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.svm import SVC, SVR
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor
from lightgbm import LGBMClassifier, LGBMRegressor
from xgboost import XGBClassifier, XGBRegressor

# métricas de clasificación
from sklearn.metrics import accuracy_score, precision_score, balanced_accuracy_score, f1_score, recall_score
from sklearn.metrics import roc_auc_score, roc_curve, classification_report, confusion_matrix, ConfusionMatrixDisplay

# métricas de regresión
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error

# Función para el preprocesado de dataframe
def preprocesar_datos(df, target):
    """
    Preprocesa un DataFrame aplicando imputación y escalado a variables numéricas,
    e imputación y codificación One-Hot a variables categóricas/booleanas.

    Args:
        df (pd.DataFrame): El DataFrame a preprocesar.
        target (str): El nombre de la columna objetivo. Si es None preprocesamos sin target

    Returns:
        pd.DataFrame: El DataFrame preprocesado.
    """   
    # Seleccionamos datos de trabajo si tenemos o no el target
    if target == "None":
      dfs = df.copy()
    else:
      dfs = df.drop(target, axis=1)

    # Identificar tipos de variables
    numeric_features = dfs.select_dtypes(include=np.number).columns
    categorical_features = dfs.select_dtypes(include=['object', 'category', 'bool']).columns
    # Número de cada tipo de variables
    lnum = len(numeric_features)
    lcat = len(categorical_features)

    # Crear pipelines para el preprocesado numérico y categórico en función del número de varaibles
    # de tipo numérico o categórico

    ### Variables de ambos tipos
    if (lnum > 0) & (lcat >0):
          # Pipeline para variables numéricas: imputación con mediana y estandarización
          numeric_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler())
          ])
          # Pipeline para variables categóricas: imputación con moda y codificación One-Hot
          categorical_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first')) # drop='first' elimina la primera categoría
           ])
          # Combinar los preprocesamientos utilizando ColumnTransformer
          preprocessor = ColumnTransformer(
            transformers=[
              ('num', numeric_transformer, numeric_features),
              ('cat', categorical_transformer, categorical_features)
           ])
          # Aplicar el preprocesado al DataFrame
          df_preprocessed = preprocessor.fit_transform(dfs)
          # Asignación de nombres de variables
          cat_feature_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features)
          all_feature_names = list(numeric_features) + list(cat_feature_names)
          df_preprocessed = pd.DataFrame(df_preprocessed, columns=all_feature_names, index=df.index)

    ### Variables de tipo numérico sin variables de tipo categórico
    if (lnum > 0) & (lcat == 0):
          # Pipeline para variables numéricas: imputación con mediana y estandarización
          numeric_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler())
          ])
           # Combinar los preprocesamientos utilizando ColumnTransformer
          preprocessor = ColumnTransformer(
            transformers=[
              ('num', numeric_transformer, numeric_features)
           ])
          # Aplicar el preprocesado al DataFrame
          df_preprocessed = preprocessor.fit_transform(dfs)
          # Asignación de nombres de variables
          all_feature_names = list(numeric_features)
          df_preprocessed = pd.DataFrame(df_preprocessed, columns=all_feature_names, index=df.index)

    ### Variables de categórico sin varaibles de tipo numérico
    if (lnum == 0) & (lcat >0):
          # Pipeline para variables categóricas: imputación con moda y codificación One-Hot
          categorical_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first')) # drop='first' elimina la primera categoría
           ])
          # Combinar los preprocesamientos utilizando ColumnTransformer
          preprocessor = ColumnTransformer(
            transformers=[
              ('cat', categorical_transformer, categorical_features)
           ])
          # Aplicar el preprocesado al DataFrame
          df_preprocessed = preprocessor.fit_transform(dfs)
          # Asignación de nombres de variables
          cat_feature_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features)
          all_feature_names = list(cat_feature_names)
          df_preprocessed = pd.DataFrame(df_preprocessed, columns=all_feature_names, index=df.index)

    if target == "None":
      datos = df_preprocessed.copy()
    else:
      datos = pd.concat([df_preprocessed, df[target]], axis=1)

    return datos

# Función para muestreo estratificado por target

def split_sample(df, target, size, semilla, estratificar):
  """
  Función para obtener la división de muestras de entrenamiento y test estratificando por un factor.

  Parámetros de entrada:
  - df: dataframe de datos completo
  - target: target por el que estratificar
  - size: porcentaje de la muestra de test
  - semilla: semilla aleatoria para la división y reproducibilidad
  - estratificar: boleano que indica si debemos estratificar por el target. 
  """
  from sklearn.model_selection import train_test_split
    
  X = df.drop(target,axis=1)
  y = df[target]

  if estratificar == True:
      print(f"Estratificando por la variable objetivo '{target}' (categórica).")
      # División de muestras con estratificación
      X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=size, random_state=semilla, stratify=y)
  else:
      print(f"No estratificando por la variable objetivo '{target}' (no categórica).")
      # División de muestras sin estratificación
      X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=size, random_state=semilla)

  # Dataframes de entrenamiento y test
  strain = pd.concat([X_train,y_train],axis=1).reset_index(drop=True)
  stest = pd.concat([X_test,y_test],axis=1).reset_index(drop=True)
  return(strain,stest)

# Función para comparar diferentes modelos de clasificación binaria

def comparar_clasificador_2cls(strain, target, sizeval, semilla, models_to_train = None):
    """
    Entrena varios modelos de clasificación y devuelve sus métricas de rendimiento.

    Args:
        strain (pd.DataFrame): Conjunto de entrenamiento.
        target (str): Nombre de la columna objetivo.
        sizeval: porcentaje del conjunto de datos que se usa para validación.
        semilla: semilla de aleatorización
        models_to_train (list, optional): Lista de nombres de modelos a entrenar.
                                          Si es None, entrena todos los modelos definidos.

    Returns:
        pd.DataFrame: DataFrame con las métricas (Precision, Recall, F1) para cada modelo.
    """

    # Definir los modelos a entrenar (conjunto completo)
    all_classifiers = {
        "lr": LogisticRegression(random_state=semilla,solver="saga"),
        "ridge": RidgeClassifier(random_state=semilla),
        "lda": LinearDiscriminantAnalysis(),
        "qda": QuadraticDiscriminantAnalysis(),
        "nb": GaussianNB(),
        "knn": KNeighborsClassifier(),
        "svc": SVC(kernel='linear', random_state=semilla),
        "rbf": SVC(kernel='rbf', random_state=semilla),
        "dt": DecisionTreeClassifier(random_state=semilla),
        "rf": RandomForestClassifier(random_state=semilla),
        "ada": AdaBoostClassifier(random_state=semilla),
        "gbc": GradientBoostingClassifier(random_state=semilla),
        "lightgbm": LGBMClassifier(random_state=semilla, verbose=-1), #verbose=-1 para evitar imprimir info de entrenamiento
        "xgboost": XGBClassifier(random_state=semilla, eval_metric='logloss') # use_label_encoder=False y eval_metric para evitar warnings
    }

    # Seleccionar los modelos a entrenar según la lista proporcionada
    if models_to_train is None:
        classifiers = all_classifiers
    else:
        classifiers = {name: all_classifiers[name] for name in models_to_train if name in all_classifiers}
        if len(classifiers) != len(models_to_train):
            print("Advertencia: Algunos nombres de modelos en la lista proporcionada no son válidos.")

    # Dividimos el conjunto entre entrenamiento y validación
    strain_df, sval_df = split_sample(strain, target, 1-sizeval, semilla, True)
    # Asignación
    X_train = strain_df.drop(target, axis=1)
    y_train = strain_df[target]
    X_val = sval_df.drop(target, axis=1)
    y_val = sval_df[target]

    # Entrenamiento y almacenamienyo de métricas
    results = []
    for name, clf in classifiers.items():
        print(f"Entrenando {name}...")
        try:
            # Entrenar el modelo
            clf.fit(X_train, y_train)
            # Predecir en los datos de entrenamiento para calcular las métricas
            y_pred = clf.predict(X_val)
            # Calcular métricas
            acc = accuracy_score(y_val, y_pred)
            balanced_acc = balanced_accuracy_score(y_val, y_pred)
            # Usar average='weighted' para métricas en problemas multiclase
            precision = precision_score(y_val, y_pred, average='weighted', zero_division=0)
            recall = recall_score(y_val, y_pred, average='weighted', zero_division=0)
            f1 = f1_score(y_val, y_pred, average='weighted', zero_division=0)
            auc = roc_auc_score(y_val, y_pred)
            results.append({'Algorithm': name, 'Accuracy': acc, 'Balanced_Accuracy': balanced_acc, 'Precision': precision, 'Recall': recall, 'F1': f1, 'AUC': auc})

        except Exception as e:
            print(f"Error entrenando {name}: {e}")
            results.append({'Algorithm': name, 'Accuracy': None, 'Balanced_Accuracy': None, 'Precision': precision, 'Recall': None, 'F1-score': None,})

    return pd.DataFrame(results)

# Función para comparar diferentes modelos de clasificación multinomial

def comparar_clasificador_multicls(strain, target, sizeval, semilla, models_to_train = None):
    """
    Entrena varios modelos de clasificación multiclase y devuelve sus métricas de rendimiento.

    Args:
        strain (pd.DataFrame): Conjunto de entrenamiento.
        target (str): Nombre de la columna objetivo.
        sizeval: porcentaje del conjunto de datos que se usa para validación.
        semilla: semilla de aleatorización
        models_to_train (list, optional): Lista de nombres de modelos a entrenar.
                                          Si es None, entrena todos los modelos definidos.

    Returns:
        pd.DataFrame: DataFrame con las métricas (Accuracy, Balanced Accuracy, Precision, Recall, F1) para cada modelo.
    """

    # Split data into features (X) and target (y)
    X = strain.drop(columns=[target])
    y = strain[target]
    # Stratified split for training and validation sets
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=sizeval, random_state=semilla, stratify=y)
  
    # Definir los modelos a entrenar (conjunto completo)

    all_models = {
        "lr": LogisticRegression(random_state=semilla, solver='liblinear'), # Usar solver compatible con multiclase
        "ridge": RidgeClassifier(random_state=semilla),
        "lda": LinearDiscriminantAnalysis(),
        "qda": QuadraticDiscriminantAnalysis(),
        "nb": GaussianNB(),
        "knn": KNeighborsClassifier(),
        "svc": SVC(kernel='linear', random_state=semilla),
        "rbf": SVC(kernel='rbf', random_state=semilla),
        "dt": DecisionTreeClassifier(random_state=semilla),
        "rf": RandomForestClassifier(random_state=semilla),
        "ada": AdaBoostClassifier(random_state=semilla),
        "gbc": GradientBoostingClassifier(random_state=semilla),
        "lightgbm": LGBMClassifier(random_state=semilla, verbose=-1),
        "xgboost": XGBClassifier(random_state=semilla, eval_metric='logloss')
    }

    if models_to_train is None:
        models = all_models
    else:
        models = {name: all_models[name] for name in models_to_train if name in all_models}

    results = []

    for name, model in models.items():
        print(f"Entrenando {name}...")
        try:
            model.fit(X_train, y_train)
            y_pred = model.predict(X_val)

            # Calculate metrics
            accuracy = accuracy_score(y_val, y_pred)
            balanced_accuracy = balanced_accuracy_score(y_val, y_pred)
            precision = precision_score(y_val, y_pred, average='weighted', zero_division=0)
            recall = recall_score(y_val, y_pred, average='weighted', zero_division=0)
            f1 = f1_score(y_val, y_pred, average='weighted', zero_division=0)

            results.append({
                'Algorithm': name,
                'Accuracy': accuracy,
                'Balanced_Accuracy': balanced_accuracy,
                'Precision': precision,
                'Recall': recall,
                'F1-score': f1
            })
        except Exception as e:
            print(f"Error entrenando {name}: {e}")
            results.append({
                'Algorithm': name,
                'Accuracy': None,
                'Balanced_Accuracy': None,
                'Precision': None,
                'Recall': None,
                'F1-score': None
            })

    return pd.DataFrame(results)


# Función para comparar diferentes modelos de regresión

def comparar_regresores(strain, target, sizeval, semilla, models_to_train=None):
    """
    Entrena varios modelos de regresión y devuelve sus métricas de rendimiento.

    Args:
        strain (pd.DataFrame): Conjunto de entrenamiento.
        target (str): Nombre de la columna objetivo.
        sizeval: porcentaje del conjunto de datos que se usa para validación.
        models_to_train (list, optional): Lista de nombres de modelos a entrenar.
                                          Si es None, entrena todos los modelos definidos.

    Returns:
        pd.DataFrame: DataFrame con las métricas (MSE, RMSE, MAE, MAPE) para cada modelo.
    """

    # Split data into features (X) and target (y)
    X = strain.drop(columns=[target])
    y = strain[target]
    # Stratified split for training and validation sets
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=sizeval, random_state=semilla)
    
    # Definir los modelos a entrenar (conjunto completo)
    all_regressors = {
        "lr": LinearRegression(),
        "lasso": Lasso(random_state=semilla),
        "ridge": Ridge(random_state=semilla),
        "knn": KNeighborsRegressor(),
        "svr": SVR(),
        "dt": DecisionTreeRegressor(random_state=semilla),
        "rf": RandomForestRegressor(random_state=semilla),
        "ada": AdaBoostRegressor(random_state=semilla),
        "gbr": GradientBoostingRegressor(random_state=semilla),
        "lightgbm": LGBMRegressor(random_state=semilla, verbose=-1), #verbose=-1 para evitar imprimir info de entrenamiento
        "xgboost": XGBRegressor(random_state=semilla, use_label_encoder=False, eval_metric='rmse') # use_label_encoder=False y eval_metric para evitar warnings
    }

    # Seleccionar los modelos a entrenar según la lista proporcionada
    if models_to_train is None:
        regressors = all_regressors
    else:
        regressors = {name: all_regressors[name] for name in models_to_train if name in all_regressors}
        if len(regressors) != len(models_to_train):
            print("Advertencia: Algunos nombres de modelos en la lista proporcionada no son válidos.")

    # Entrenamiento y almacenamienyo de métricas
    results = []

    for name, reg in regressors.items():
        print(f"Entrenando {name}...")
        try:
            # Entrenar el modelo
            reg.fit(X_train, y_train)

            # Predecir en los datos de entrenamiento para calcular las métricas
            y_pred = reg.predict(X_val)

            # Calcular métricas
            mse = mean_squared_error(y_val, y_pred)
            rmse = np.sqrt(mse)
            mae = mean_absolute_error(y_val, y_pred)
            # Manejar el caso de división por cero en MAPE si hay valores de y_train igual a cero
            mape = mean_absolute_percentage_error(y_val, y_pred) if not (y_val == 0).any() else np.nan

            results.append({'Algorithm': name, 'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'MAPE': mape})

        except Exception as e:
            print(f"Error entrenando {name}: {e}")
            results.append({'Algorithm': name, 'MSE': None, 'RMSE': None, 'MAE': None, 'MAPE': None})

    return pd.DataFrame(results)

def reports_clas(modelo, xtrain, ytrain, xtest, ytest):
  '''
  Función para obtener el report de clasificación para un modelo de clasifcación. Porpociona los resultados para la muestra de entrenamiento y test

  Argumentos:
  - modelo: modelo entrenado
  - xtrain: inputs de entrenamiento
  - ytrain: target de entrenamiento
  - xtest: inputs de test
  - ytest: target de test

  Return
   - reporte de clasificación para muestra de entrenamiento y test
  '''
  from sklearn.metrics import classification_report

  clase_train = modelo.predict(xtrain)
  clase_test = modelo.predict(xtest)
  print("Métricas de clasificación en la muestra de entrenamiento")
  print(classification_report(ytrain, clase_train))
  print("\n Métricas de clasificación en la muestra test")
  print(classification_report(ytest, clase_test))

def matriz_confusion(modelo, xtest, ytest):
  '''
  Función que proporciona la matriz de confusión de un modelo de clasificación en términos de los porcentajes de clasificación correcta dentro de cada clase

  Argumentos de entrada:
  - modelo: modelo entrenado
  - xtest: inputs de test
  - ytest: target de test

  Return:
  - Matriz de confusión
  '''
  from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

  pred_test = modelo.predict(xtest)
  # Matriz de confusión para datos de test
  cm = confusion_matrix(ytest, pred_test, labels = modelo.classes_)
  # Normalizamos para representar porcentajes en lugar de frecuencias
  cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
  # Solución gráfica
  disp = ConfusionMatrixDisplay(confusion_matrix = cm_normalized, display_labels = modelo.classes_)
  disp = disp.plot(cmap = plt.cm.Blues, values_format ='.2%')
  plt.grid(False)
  plt.show()

def validar_modelo(modelo, xtrain, ytrain, score, folds):
  '''
  Función que proporciona un análisis de validación cruzada con respecto a un score de evaluación

  Argumentos de entrada:
  - modelo: modelo entrenado
  - xtrain: inputs de entrenamiento
  - ytrain: target de entrenamiento
  - score: métrica de evaluación 
  - folds: número de folds de validación cruzada

  Return:
  - tabla con el análisis de validación cruzada
  '''

  from sklearn.model_selection import cross_val_score
  from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score

  # Determine the scoring strategy based on the 'score' parameter
  if score == 'precision':
    scorer = make_scorer(precision_score, average='weighted', zero_division=0)
  elif score == 'recall':
    scorer = make_scorer(recall_score, average='weighted', zero_division=0)
  elif score == 'f1':
    scorer = make_scorer(f1_score, average='weighted', zero_division=0)
  else:
    scorer = score # Use the original score string for other metrics

  # Análsiis de validación cruzada
  score_val = pd.DataFrame(cross_val_score(modelo, xtrain, ytrain, cv = folds,
                                         scoring = scorer),
                                         columns=['score'])
  print("Análisis de validación cruzada")
  print(score_val.describe().T)

def curva_aprendizaje(modelo, X, y, score, folds):
  '''
  Función que proporciona un análisis de validación cruzada con respecto a un score de evaluación

  Argumentos de entrada:
  - modelo: modelo entrenado
  - X: inputs
  - y: target
  - score: métrica de evaluación ('accuracy','recall','f1')
  - folds: número de folds de validación cruzada

  Return:
  - tabla con el análisis de validación cruzada
  '''
  from sklearn.model_selection import cross_val_score, learning_curve

  #Fijamos tamaños de muestra de entrenamiento
  size = np.arange(0.2, 0.91, 0.1)
  # Evaluamos la exactitud en la muestra test para los diferentes tamaños
  train_sizes, train_scores, test_scores = learning_curve(modelo, X, y,
                                                        train_sizes = size,
                                                        scoring = score,
                                                        cv = folds)
  # Representamos gráficamente
  plt.plot(size, test_scores.mean(1), "o--", color="r")
  plt.xlabel("Proporción muestra entrenamiento")
  plt.ylabel("Promedio score")
  plt.title("Curva de aprendizaje")
  plt.grid(True)
  plt.show()

def select_variables(modelo, X, y, k_values, score):
    """
    Selecciona las k mejores características utilizando la Eliminación Recursiva de Características (RFE)
    con validación cruzada y devuelve puntuaciones para cada k y las mejores características.

    Args:
        modelo: modelo de clasificación o regresión.
        X (pd.DataFrame): matriz de inputs
        y (pd.Series): vector de target
        k_values (list): lista de valores de k donde debemos buscar la mejor combinación.
        score (str): métrica de evaluación para la validación cruzada.

    Returns:
        tupla: una tupla que contien:
            - pd.DataFrame: un dataframe con los valores de k y el score obtenido.
            - list: una lista con los nomres de los inputs seleccionados
            - int: El valor de k óptimo.
            - float: El valor del score de validación cruzada para el mejor valor de k.
    """
    from sklearn.feature_selection import RFE
    from sklearn.model_selection import cross_val_score

    best_k = -1
    best_score = -float('inf')
    best_features = None
    scores_list = []

    for k in k_values:
        # Nos aseguramos que el valor de k no exce el número de columnas de X
        if k > X.shape[1]:
            print(f"Cuidado: k={k} es mayor que el número de columnas de X ({X.shape[1]}).")
            continue

        selector = RFE(modelo, n_features_to_select=k)
        X_new = selector.fit_transform(X, y)

        # Evaluamos el comportamiento del modelo completo
        scores = cross_val_score(modelo, X_new, y, cv =10, scoring = score)
        mean_score = scores.mean()

        scores_list.append({'k': k, 'score': mean_score})

        if mean_score > best_score:
            best_score = mean_score
            best_k = k
            # Get the names of the selected features
            best_features = X.columns[selector.support_].tolist()

    scores_df = pd.DataFrame(scores_list)

    return scores_df, best_features, best_k, best_score


def podar_tree_clf(modelo,xtrain,ytrain,xtest,ytest):
  '''
  Función para llevar a cabo la poda de un árbol de decisión

  Argumentos:
    - modelo: modelo de árbol de decisión
    - xtrain: muestras de entrenamiento
    - ytrain: etiquetas de entrenamiento
    - xtest: muestras de test
    - ytest: etiquetas de test

  Devuelve:
    Imprimer el valor de alpha óptimo y devuelve el árbol podado
  '''
  from sklearn import tree
  from sklearn.tree import DecisionTreeClassifier
  
  # Identificamos los valores alpha e impurezas
  path = modelo.cost_complexity_pruning_path(xtrain, ytrain)
  # extraer diferentes valores de alpha (y excluimos el del árbol trivial)
  ccp_alphas = path.ccp_alphas[:-1]
  # creamos un array donde introduciremos los árboles de decisión
  clfs = []
  # creamos un árbol de decisión por cada valor de alpha y lo guardamos
  for ccp_alpha in ccp_alphas:
    clf = DecisionTreeClassifier(random_state = 0, ccp_alpha = ccp_alpha)
    clf.fit(xtrain, ytrain)
    clfs.append(clf)
  # Obtenemos la exactitud en las muestras train/test
  train_scores = [clfsco.score(xtrain, ytrain) for clfsco in clfs]
  test_scores = [clfsco.score(xtest, ytest) for clfsco in clfs]
  # guardamos todos los resultados
  poda = pd.DataFrame(np.stack([ccp_alphas,
              np.array(train_scores),
              np.array(test_scores)],axis=1),
             columns= ["alpha", "score_train", "score_test"])
  # identificamos la exactitud máxima en la muestra test
  pos = poda.idxmax(axis=0).score_test
  # Valores óptimos
  print(f"Valor óptimo de alpha: {poda.iloc[pos,0]:.4f}")
  print(f"Score muestra de entrenamiento: {poda.iloc[pos,1]:.4f}")
  print(f"Score muestra de test: {poda.iloc[pos,2]:.4f}")

  modelofinal = DecisionTreeClassifier(random_state=0,ccp_alpha=poda.iloc[pos,0])
  return(modelofinal)

def plot_importancias(modelo):
  '''
  Función para obtener las importancias de los inputs de un modelo

  Argumentos:
  - modelo: modelo entrenado

  Devuelve:
  - Imprime matriz de importancia y devuleve el gráfico asociado
  '''
  matriz_importancia= pd.DataFrame([modelo.feature_importances_],
             index = ["importancia"],
             columns = modelo.feature_names_in_).T
  matriz_importancia.sort_values(by='importancia',ascending=False,inplace=True)
  print("Matriz de importancias")
  print(matriz_importancia)
  matriz_importancia.plot.barh();

def grafico_prediccion(modelo, xtest, ytest):
    """
    Genera un gráfico de dispersión comparando los valores reales (ytest)
    contra los valores predichos por el modelo.

    Parámetros:
    - modelo: El modelo ya entrenado (ej. LinearRegression).
    - xtest: Datos de características para prueba.
    - ytest: Etiquetas reales correspondientes a xtest.
    """
    # 1. Generar las predicciones
    ypred = modelo.predict(xtest)

    # 2. Configurar el tamaño de la figura
    plt.figure(figsize=(8,8))

    # 3. Crear el gráfico de dispersión (Scatter Plot)
    # alpha=0.6 ayuda a ver dónde se concentran los datos si hay muchos puntos solapados
    plt.scatter(ypred, ytest, alpha=0.6, color='royalblue', edgecolor='k', s=50)

    # 4. Añadir línea de referencia (Identidad perfect y=x)
    # Calculamos el mínimo y máximo global para trazar la línea diagonal completa
    min_val = min(np.min(ytest), np.min(ypred))
    max_val = max(np.max(ytest), np.max(ypred))
    plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Predicción Perfecta')

    # 5. Etiquetas y Títulos
    plt.xlabel('Valor Predicho', fontsize=12)
    plt.ylabel('Valor Real', fontsize=12)
    plt.title('')
    plt.legend()
    plt.grid(True, linestyle=':', alpha=0.6)

    # Mostrar gráfico
    plt.show()

import scipy.stats as stats
from sklearn.metrics import mean_squared_error, r2_score

def evaluar_modelo_regresion(modelo, xtrain, ytrain, xtest, ytest):
    """
    Calcula y devuelve un DataFrame con RMSE, R2 Ajustado, F-statistic, P-valor,
    AIC y BIC comparando los conjuntos de Train y Test.

    Parámetros:
    - modelo: El modelo ya entrenado.
    - xtrain: Datos de características para train.
    - ytrain: Etiquetas reales correspondientes a xtrain.
    - xtest: Datos de características para test.
    - ytest: Etiquetas reales correspondientes a xtest.
    """

    resultados = {}

    # Iteramos sobre los dos conjuntos (Train y Test)
    datas = [('Train', xtrain, ytrain), ('Test', xtest, ytest)]

    for nombre, x, y in datas:
        # 1. Predicciones
        ypred = modelo.predict(x)

        # 2. Variables auxiliares
        n = len(y)          # Número de observaciones
        p = x.shape[1]      # Número de predictores

        # 3. RMSE
        mse = mean_squared_error(y, ypred)
        rmse = np.sqrt(mse)

        # 4. R2 y R2 Ajustado
        r2 = r2_score(y, ypred)
        r2_adj = 1 - (1 - r2) * (n - 1) / (n - p - 1)

        # 5. Estadístico F y P-valor
        if r2 > 0 and r2 < 1:
            f_stat = (r2 / p) / ((1 - r2) / (n - p - 1))
            p_val = stats.f.sf(f_stat, p, n - p - 1)
        else:
            f_stat = np.nan
            p_val = np.nan

        # 6. AIC y BIC
        # Cálculo manual basado en la Log-Verosimilitud aproximada usando MSE
        # k = número de parámetros (predictores + intercepto)
        k = p + 1

        # Fórmula: AIC = n * ln(MSE) + 2k
        aic = n * np.log(mse) + 2 * k

        # Fórmula: BIC = n * ln(MSE) + k * ln(n)
        bic = n * np.log(mse) + k * np.log(n)

        # Guardamos en el diccionario (Asegurando que coincida con el índice de abajo)
        resultados[nombre] = [rmse, r2_adj, p_val, aic, bic]

    # Crear DataFrame
    indices = [
        'RMSE',
        'R2 Ajustado',
        'P-Valor F-Stat',
        'AIC',
        'BIC'
    ]

    df_metricas = pd.DataFrame(resultados, index=indices)
    return df_metricas

from sklearn.model_selection import cross_val_score
from sklearn.metrics import make_scorer, mean_squared_error, r2_score

def validar_modelo_regresion(modelo, xtrain, ytrain, score, folds):
    '''
    Función que proporciona un análisis de validación cruzada para regresión 
    con respecto a un score de evaluación.

    Argumentos de entrada:
    - modelo: modelo instanciado o entrenado (sklearn estimator)
    - xtrain: inputs de entrenamiento (DataFrame o numpy array)
    - ytrain: target de entrenamiento
    - score: métrica de evaluación ('r2', 'rmse', 'r2_ajustado', 'neg_mean_absolute_error', etc.)
    - folds: número de folds de validación cruzada

    Return:
    - tabla (DataFrame) con el análisis descriptivo de la validación cruzada
    '''

    # 1. Definimos las funciones de métricas personalizadas
    # Es importante definirlas dentro para que capturen el contexto si fuera necesario
    
    def rmse_calc(y_true, y_pred):
        """Calcula Root Mean Squared Error"""
        return np.sqrt(mean_squared_error(y_true, y_pred))

    def r2_adj_calc(y_true, y_pred):
        """Calcula R2 Ajustado"""
        n = len(y_true)
        # Obtenemos p (número de predictores) de la variable xtrain pasada a la función padre
        p = xtrain.shape[1] 
        r2 = r2_score(y_true, y_pred)
        return 1 - (1 - r2) * (n - 1) / (n - p - 1)

    # 2. Determinamos la estrategia de scoring
    if score == 'rmse':
        # Creamos un scorer que devuelva el error positivo
        scorer = make_scorer(rmse_calc) 
        nombre_metrica = "RMSE"
        
    elif score == 'r2_ajustado':
        # Creamos el scorer para R2 ajustado
        scorer = make_scorer(r2_adj_calc)
        nombre_metrica = "R2 Ajustado"
        
    else:
        # Si no es personalizado, usamos el string de scikit-learn (ej: 'r2', 'neg_mean_squared_error')
        scorer = score 
        nombre_metrica = score

    # 3. Ejecución de la Validación Cruzada
    try:
        cv_scores = cross_val_score(estimator=modelo, 
                                    X=xtrain, 
                                    y=ytrain, 
                                    cv=folds, 
                                    scoring=scorer)
    except ValueError as e:
        return f"Error al ejecutar cross_val_score. Verifica la métrica o los datos: {e}"

    # 4. Formateo de resultados
    # Convertimos a DataFrame para usar .describe()
    score_val = pd.DataFrame(cv_scores, columns=[nombre_metrica])
    
    # Obtenemos las estadísticas descriptivas y transponemos
    # Nota: Corregí la sintaxis del round() que tenía un error en tu snippet original
    tabla_resultado = score_val.describe().T.round(4)
    tabla_resulatdo

import pandas as pd
import numpy as np
from sklearn.feature_selection import RFE
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error

def select_variables_regresion(modelo, X, y, k_values, score):
    """
    1. Ordena todas las predictoras por importancia usando RFE.
    2. Evalúa iterativamente los modelos incrementando k.
    3. Devuelve las variables ordenadas de mayor a menor importancia según RFE.

    Args:
        modelo: modelo de regresión.
        X (pd.DataFrame): matriz de inputs.
        y (pd.Series): vector de target.
        k_values (list): lista de valores de k a probar.
        score (str): métrica para optimizar ('rmse', 'aic', 'bic').

    Returns:
        tupla: 
            - pd.DataFrame: Tabla con k, métricas y variables acumuladas.
            - list: Las mejores variables ORDENADAS por importancia.
            - int: k óptimo.
            - float: score óptimo.
    """
    
    # Validación del score
    valid_scores = ['rmse', 'aic', 'bic']
    if score not in valid_scores:
        raise ValueError(f"El parámetro 'score' debe ser uno de: {valid_scores}")

    print("Calculando el ranking global de variables (puede tardar un momento)...")
    
    # 1. OBTENER EL ORDEN DE IMPORTANCIA GLOBAL (Ranking)
    # Al pedirle que seleccione 1, RFE calculará el ranking de eliminación de TODAS.
    # La variable con ranking 1 es la última en ser eliminada (la más importante).
    rfe_global = RFE(modelo, n_features_to_select=1)
    rfe_global.fit(X, y)

    # Creamos un DataFrame para ordenar las variables según su ranking RFE
    mapa_ranking = pd.DataFrame({
        'Variable': X.columns,
        'Ranking': rfe_global.ranking_
    }).sort_values('Ranking') # Orden ascendente: 1 (mejor), 2 (segunda), ...

    # Lista maestra ordenada de la más importante a la menos importante
    variables_ordenadas = mapa_ranking['Variable'].tolist()
    
    # -------------------------------------------------------------------------
    
    best_k = -1
    best_score = float('inf')
    best_features = None
    scores_list = []
    n = len(y)

    print(f"Evaluando combinaciones optimizando por: {score.upper()}...\n")

    # 2. EVALUACIÓN ITERATIVA SIGUIENDO EL ORDEN
    for k in k_values:
        if k > len(variables_ordenadas): continue

        # Seleccionamos las top k variables de la lista ya ordenada
        current_vars = variables_ordenadas[:k]
        X_new = X[current_vars]
        
        # --- CÁLCULO DE MÉTRICAS ---
        
        # A) RMSE (Validación Cruzada)
        cv_scores = cross_val_score(modelo, X_new, y, cv=10, scoring='neg_root_mean_squared_error')
        rmse_val = -cv_scores.mean()

        # B) AIC y BIC (Ajuste global)
        modelo.fit(X_new, y)
        ypred = modelo.predict(X_new)
        mse = mean_squared_error(y, ypred)
        num_params = k + 1 
        
        aic_val = n * np.log(mse) + 2 * num_params
        bic_val = n * np.log(mse) + num_params * np.log(n)

        # Guardamos métricas
        metrics = {
            'k': k, 
            'rmse': rmse_val, 
            'aic': aic_val, 
            'bic': bic_val,
            # Mostramos la última variable agregada para ver qué aporta
            'Nueva Variable': current_vars[-1] 
        }
        scores_list.append(metrics)

        # --- OPTIMIZACIÓN ---
        current_score_value = metrics[score]

        if current_score_value < best_score:
            best_score = current_score_value
            best_k = k
            # Guardamos la lista tal cual (ya está ordenada por importancia)
            best_features = current_vars

    scores_df = pd.DataFrame(scores_list)

    return scores_df, best_features, best_k, best_score

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve

def curva_aprendizaje_regresion(modelo, X, y, score, folds):
    '''
    Genera una curva de aprendizaje para modelos de regresión.
    Muestra cómo evoluciona la métrica de validación al aumentar el tamaño del set de entrenamiento.
    '''

    # 1. Configuración de la métrica
    if score == 'rmse':
        sklearn_score = 'neg_root_mean_squared_error'
    elif score == 'r2_ajustado':
        sklearn_score = 'r2'
    else:
        sklearn_score = score

    # --- CORRECCIÓN AQUÍ ---
    # Usamos linspace para asegurar que el máximo sea exactamente 1.0 y no 1.00000001
    # Esto genera 5 puntos equidistantes entre el 20% y el 100%
    sizes_prop = np.linspace(0.2, 1.0, 5) 

    # 2. Calcular curva de aprendizaje
    train_sizes_abs, train_scores, test_scores = learning_curve(
        modelo, 
        X, 
        y, 
        train_sizes=sizes_prop,
        scoring=sklearn_score,
        cv=folds,
        n_jobs=-1 
    )

    # 3. Procesamiento de resultados
    mean_test_scores = test_scores.mean(axis=1)

    if score == 'rmse':
        mean_test_scores = -mean_test_scores
        titulo_y = "RMSE (Menor es mejor)"
    elif score == 'r2_ajustado':
        p = X.shape[1] 
        # Fórmula vectorizada para ajustar R2
        mean_test_scores = 1 - (1 - mean_test_scores) * (train_sizes_abs - 1) / (train_sizes_abs - p - 1)
        titulo_y = "R2 Ajustado (Mayor es mejor)"
    else:
        titulo_y = f"Score promedio ({score})"

    # 4. Gráfico
    plt.figure(figsize=(8, 5))
    plt.plot(sizes_prop, mean_test_scores, "o--", color="r", label="Validación (CV)")
    
    plt.xlabel("Proporción muestra entrenamiento")
    plt.ylabel(titulo_y)
    plt.title(f"Curva de Aprendizaje - {score.upper()}")
    plt.legend()
    plt.grid(True)
    plt.show()
